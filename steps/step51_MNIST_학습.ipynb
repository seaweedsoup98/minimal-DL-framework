{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22096fcc",
   "metadata": {},
   "source": [
    "## 51단계: MNIST 학습\n",
    "\n",
    "> 간단히 복습하면, 우선 Dataset 클래스로 데이터셋 처리를 위한 공통 인터페이스를 마련했고, '전처리'를 설정할 수 있도록 했습니다. 그리고 DataLoader 클래스로는 Dataset에서 미니배치 단위로 데이터를 꺼낼 수 있게 했습니다. 이상의 관계를 `그림 51-1`로 표현할 수 있습니다. \\\n",
    "이번 단계에서는 이상의 데이터셋 구조를 활용하여 MNIST 데이터셋을 학습할 것입니다.\n",
    "\n",
    "<img src=\"images/그림 51-1.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e289a",
   "metadata": {},
   "source": [
    "### 51.1 MNIST 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b53fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_set) 60000\n",
      "len(test_set) 10000\n",
      "type(x), x.shape <class 'numpy.ndarray'> (1, 28, 28)\n",
      "label 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=None)\n",
    "\n",
    "print(\"len(train_set)\", len(train_set))\n",
    "print(\"len(test_set)\", len(test_set))\n",
    "\n",
    "x, t = train_set[0]\n",
    "print(\"type(x), x.shape\", type(x), x.shape)\n",
    "print(\"label\", t)\n",
    "\n",
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb7de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 추가\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=f)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=f)\n",
    "\n",
    "# 다만, MNIST 클래스 내부에 transform을 정의해뒀기 때문에\n",
    "# transform 인자를 지정하지 않으면 자동으로 전처리가 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f00d4",
   "metadata": {},
   "source": [
    "### 51.2 MNIST 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b2dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.9097, accuracy: 0.5569\n",
      "test loss: 1.5337, accuracy: 0.7324\n",
      "epoch: 2\n",
      "train loss: 1.2796, accuracy: 0.7728\n",
      "test loss: 1.0368, accuracy: 0.8102\n",
      "epoch: 3\n",
      "train loss: 0.9212, accuracy: 0.8186\n",
      "test loss: 0.7898, accuracy: 0.8445\n",
      "epoch: 4\n",
      "train loss: 0.7376, accuracy: 0.8411\n",
      "test loss: 0.6551, accuracy: 0.8556\n",
      "epoch: 5\n",
      "train loss: 0.6336, accuracy: 0.8545\n",
      "test loss: 0.5743, accuracy: 0.8680\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "import dezero.functions as F\n",
    "from dezero import optimizers\n",
    "from dezero import DataLoader\n",
    "from dezero.models import MLP\n",
    "\n",
    "\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "test_set = dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce9470",
   "metadata": {},
   "source": [
    "### 51.3 모델 개선하기\n",
    "\n",
    "ReLU를 사용해보자.\n",
    "\n",
    "<img src=\"images/식 51.3.png\" width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b106c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/functions.py\n",
    "\n",
    "from dezero import Function\n",
    "\n",
    "class ReLU(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.maximum(x, 0.0)  # x의 각 원소와 0.0 중 큰 값을 반환\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = x.data > 0  # 출력 쪽 기울기를 '통과시킬지' 표시\n",
    "        gx = gy * mask  # mask와 기울기를 곱해줌\n",
    "        return gx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392f315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 0.1912, accuracy: 0.9421\n",
      "test loss: 0.1056, accuracy: 0.9661\n",
      "epoch: 2\n",
      "train loss: 0.0739, accuracy: 0.9766\n",
      "test loss: 0.0875, accuracy: 0.9728\n",
      "epoch: 3\n",
      "train loss: 0.0493, accuracy: 0.9840\n",
      "test loss: 0.0953, accuracy: 0.9710\n",
      "epoch: 4\n",
      "train loss: 0.0349, accuracy: 0.9888\n",
      "test loss: 0.0701, accuracy: 0.9788\n",
      "epoch: 5\n",
      "train loss: 0.0293, accuracy: 0.9906\n",
      "test loss: 0.0717, accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "test_set = dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "# model = MLP((hidden_size, 10))\n",
    "# optimizer = optimizers.SGD().setup(model)\n",
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
    "optimizer = optimizers.Adam().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
